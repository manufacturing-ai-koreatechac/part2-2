{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– Vision Transformer (ViT) ì†Œê°œ\n",
    "\n",
    "> **í•™ìŠµ ëª©í‘œ**: ViT ì•„í‚¤í…ì²˜ë¥¼ ì´í•´í•˜ê³  pre-trained ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‹¤ìŠµ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ í•™ìŠµ ë‚´ìš©\n",
    "\n",
    "1. âœ… ViT ì•„í‚¤í…ì²˜ ê°œë… (Patch Embedding, Position Encoding)\n",
    "2. âœ… Self-Attention Mechanism ì‹œê°í™”\n",
    "3. âœ… Pre-trained ViT ëª¨ë¸ ì¶”ë¡ \n",
    "4. âœ… ResNet50 vs ViT-Base ì„±ëŠ¥ ë¹„êµ\n",
    "5. âœ… Multi-scale Inference (224, 384, 512)\n",
    "6. âœ… Attention Map ì‹œê°í™” (12 layers)\n",
    "\n",
    "**ì†Œìš” ì‹œê°„**: ì•½ 45ë¶„  \n",
    "**ë‚œì´ë„**: â­â­â­ (ì¤‘ìƒê¸‰)  \n",
    "**ì‚¬ì „ ì§€ì‹**: CNN ê¸°ì´ˆ, Transformer ê°œë…\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "try:\n",
    "    from transformers import ViTForImageClassification, ViTImageProcessor, ViTConfig\n",
    "    import transformers\n",
    "    print(f\"âœ… Transformers ë²„ì „: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Transformers ì„¤ì¹˜ í•„ìš”: pip install transformers\")\n",
    "\n",
    "# ì´ë¯¸ì§€ ì²˜ë¦¬\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# ë”¥ëŸ¬ë‹\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(f\"âœ… PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "\n",
    "# ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ìœ í‹¸ë¦¬í‹°\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"\\nâœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‚ Step 2: ë°ì´í„° ë¡œë“œ\n",
    "\n",
    "**Dataset_Machinevision.zip** ì••ì¶• í•´ì œ í›„ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •\n# 1ìˆœìœ„: KAMP ì‹¤ì œ ë°ì´í„° (ì‹œì§€íŠ¸ë¡œë‹‰ìŠ¤ ì´ë¯¸ì§€)\n# 2ìˆœìœ„: ë¡œì»¬ data/ í´ë”\n# 3ìˆœìœ„: í•©ì„± ìƒ˜í”Œ ì´ë¯¸ì§€ ìë™ ìƒì„±\n\nkamp_dir = Path('../../dataset/part2-2/ì‹œì§€íŠ¸ë¡œë‹‰ìŠ¤_ì´ë¯¸ì§€')\nlocal_dir = Path('../data/MachineVision')\n\nif kamp_dir.exists() and any(kamp_dir.iterdir()):\n    data_dir = kamp_dir\n    data_source = \"KAMP ì‹¤ì œ ë°ì´í„°\"\nelif local_dir.exists() and any(local_dir.rglob('*.png')) or local_dir.exists() and any(local_dir.rglob('*.jpg')):\n    data_dir = local_dir\n    data_source = \"ë¡œì»¬ ë°ì´í„°\"\nelse:\n    data_dir = local_dir\n    data_source = \"í•©ì„± ìƒ˜í”Œ (ìë™ ìƒì„±)\"\n\n# ì´ë¯¸ì§€ íŒŒì¼ ê²€ìƒ‰\nimage_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\nimage_files = []\nfor ext in image_extensions:\n    image_files.extend(list(data_dir.rglob(ext)))\n\nprint(f\"ğŸ“‚ ë°ì´í„° ê²½ë¡œ: {data_dir}\")\nprint(f\"ğŸ“¦ ë°ì´í„° ì†ŒìŠ¤: {data_source}\")\nprint(f\"ğŸ–¼ï¸ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: {len(image_files):,}ê°œ\")\n\n# ìƒ˜í”Œ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ìƒì„±\nif len(image_files) == 0:\n    print(\"\\nâš ï¸ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±...\")\n    data_dir.mkdir(exist_ok=True, parents=True)\n    \n    # ëœë¤ ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±\n    img_array = np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8)\n    img = Image.fromarray(img_array)\n    sample_path = data_dir / 'sample.png'\n    img.save(sample_path)\n    image_files = [sample_path]\n    print(f\"âœ… ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±: {sample_path}\")\nelse:\n    print(f\"\\nğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°:\")\n    subdirs = sorted(set([f.parent.name for f in image_files]))\n    for subdir in subdirs[:10]:\n        count = len([f for f in image_files if f.parent.name == subdir])\n        print(f\"   - {subdir}: {count:,}ê°œ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œ ì´ë¯¸ì§€ ë¡œë“œ ë° ì‹œê°í™”\n",
    "sample_images = image_files[:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, img_path in enumerate(sample_images):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f'{img_path.parent.name}\\n{img.size[0]}Ã—{img.size[1]}', fontsize=10)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('ìƒ˜í”Œ ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸°', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… ìƒ˜í”Œ ì´ë¯¸ì§€ {len(sample_images)}ê°œ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 3: ViT ëª¨ë¸ ë¡œë“œ\n",
    "\n",
    "**google/vit-base-patch16-224**: ImageNet-21k pre-trained, ImageNet-1k fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "\n",
    "print(f\"ğŸ”„ ViT ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}...\")\n",
    "\n",
    "try:\n",
    "    model = ViTForImageClassification.from_pretrained(model_name)\n",
    "    processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "    config = model.config\n",
    "    \n",
    "    print(f\"\\nâœ… ViT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    print(f\"\\nğŸ“Š ëª¨ë¸ êµ¬ì¡°:\")\n",
    "    print(f\"   - íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,}ê°œ\")\n",
    "    print(f\"   - Hidden size: {config.hidden_size}\")\n",
    "    print(f\"   - Attention heads: {config.num_attention_heads}\")\n",
    "    print(f\"   - Transformer layers: {config.num_hidden_layers}\")\n",
    "    print(f\"   - Patch size: {config.patch_size}Ã—{config.patch_size}\")\n",
    "    print(f\"   - Image size: {config.image_size}Ã—{config.image_size}\")\n",
    "    print(f\"   - í´ë˜ìŠ¤ ìˆ˜: {config.num_labels}\")\n",
    "    \n",
    "    # íŒ¨ì¹˜ ìˆ˜ ê³„ì‚°\n",
    "    num_patches = (config.image_size // config.patch_size) ** 2\n",
    "    print(f\"   - Patch ê°œìˆ˜: {num_patches}ê°œ ({config.image_size // config.patch_size}Ã—{config.image_size // config.patch_size})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"   transformers ì„¤ì¹˜: pip install transformers\")\n",
    "\n",
    "# GPU ì‚¬ìš© ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"\\nğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Step 4: Patch Embedding ì‹œê°í™”\n",
    "\n",
    "ì´ë¯¸ì§€ë¥¼ 16Ã—16 íŒ¨ì¹˜ë¡œ ë¶„í• í•˜ê³  ì„ë² ë”©í•˜ëŠ” ê³¼ì • ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ìƒ˜í”Œ ì´ë¯¸ì§€ ì„ íƒ\nsample_img = Image.open(image_files[0]).convert('RGB')\nsample_img = sample_img.resize((224, 224))\n\n# íŒ¨ì¹˜ ë¶„í•  ì‹œê°í™”\npatch_size = config.patch_size\nimg_array = np.array(sample_img)\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\n\n# ì›ë³¸ ì´ë¯¸ì§€\naxes[0, 0].imshow(img_array)\naxes[0, 0].set_title('ì›ë³¸ ì´ë¯¸ì§€ (224Ã—224)', fontsize=12, fontweight='bold')\naxes[0, 0].axis('off')\n\n# íŒ¨ì¹˜ ê·¸ë¦¬ë“œ ì˜¤ë²„ë ˆì´\naxes[0, 1].imshow(img_array)\nfor i in range(0, 224, patch_size):\n    axes[0, 1].axhline(i, color='red', linewidth=1, alpha=0.5)\n    axes[0, 1].axvline(i, color='red', linewidth=1, alpha=0.5)\naxes[0, 1].set_title(f'Patch Grid ({patch_size}Ã—{patch_size})', fontsize=12, fontweight='bold')\naxes[0, 1].axis('off')\n\n# ê°œë³„ íŒ¨ì¹˜ ìƒ˜í”Œ (4ê°œ â€” 2Ã—3 ê·¸ë¦¬ë“œì˜ ë‚¨ì€ 4ì¹¸ì— ë°°ì¹˜)\npatch_indices = [(0, 0), (0, 7), (7, 0), (7, 7)]\n\nfor idx, (row, col) in enumerate(patch_indices):\n    ax_row = (idx + 2) // 3\n    ax_col = (idx + 2) % 3\n    ax = axes[ax_row, ax_col]\n    \n    y_start = row * patch_size\n    x_start = col * patch_size\n    patch = img_array[y_start:y_start+patch_size, x_start:x_start+patch_size]\n    \n    ax.imshow(patch)\n    ax.set_title(f'Patch ({row},{col})', fontsize=10)\n    ax.axis('off')\n\nplt.suptitle('ViT Patch Embedding ê³¼ì •', fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nğŸ’¡ Patch Embedding ì„¤ëª…:\")\nprint(f\"   1. ì´ë¯¸ì§€ë¥¼ {patch_size}Ã—{patch_size} íŒ¨ì¹˜ë¡œ ë¶„í•  â†’ {num_patches}ê°œ íŒ¨ì¹˜\")\nprint(f\"   2. ê° íŒ¨ì¹˜ë¥¼ 1D ë²¡í„°ë¡œ flatten â†’ {patch_size * patch_size * 3}ì°¨ì›\")\nprint(f\"   3. Linear projection â†’ {config.hidden_size}ì°¨ì› ì„ë² ë”©\")\nprint(f\"   4. Position encoding ì¶”ê°€ â†’ Transformer ì…ë ¥\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 5: ì¶”ë¡  ë° ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
    "inputs = processor(images=sample_img, return_tensors='pt')\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# ì¶”ë¡ \n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "    logits = outputs.logits\n",
    "    attentions = outputs.attentions  # 12 layers attention weights\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "top5_probs, top5_indices = torch.topk(probs, k=5)\n",
    "\n",
    "print(\"ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼ (Top 5):\")\n",
    "print(f\"{'ìˆœìœ„':<5} {'í´ë˜ìŠ¤ ID':<10} {'ì‹ ë¢°ë„':<10}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for rank, (prob, idx) in enumerate(zip(top5_probs[0], top5_indices[0]), 1):\n",
    "    print(f\"{rank:<5} {idx.item():<10} {prob.item():.2%}\")\n",
    "\n",
    "predicted_class = top5_indices[0][0].item()\n",
    "confidence = top5_probs[0][0].item()\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… ì˜ˆì¸¡: í´ë˜ìŠ¤ {predicted_class} (ì‹ ë¢°ë„: {confidence:.2%})\")\n",
    "print(f\"\\nğŸ“Š Attention ì •ë³´:\")\n",
    "print(f\"   - Attention layers: {len(attentions)}ê°œ\")\n",
    "print(f\"   - Attention shape: {attentions[0].shape}\")  # (batch, heads, seq, seq)\n",
    "print(f\"   - Attention heads: {attentions[0].shape[1]}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Step 6: Attention Map ì‹œê°í™”\n",
    "\n",
    "12ê°œ Transformer layerì˜ attention pattern ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention map ì‹œê°í™” (ì²« ë²ˆì§¸ í—¤ë“œ, CLS í† í°ì— ëŒ€í•œ attention)\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for layer_idx in range(12):\n",
    "    # CLS í† í° (ì²« ë²ˆì§¸ í† í°)ì˜ attention\n",
    "    attention = attentions[layer_idx][0, 0, 0, 1:].detach().cpu().numpy()  # [0, head=0, cls_token, patches]\n",
    "    \n",
    "    # Reshape to 2D grid\n",
    "    grid_size = int(np.sqrt(len(attention)))\n",
    "    attention_map = attention.reshape(grid_size, grid_size)\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    im = axes[layer_idx].imshow(attention_map, cmap='hot', interpolation='nearest')\n",
    "    axes[layer_idx].set_title(f'Layer {layer_idx + 1}', fontsize=10, fontweight='bold')\n",
    "    axes[layer_idx].axis('off')\n",
    "\n",
    "plt.suptitle('ViT Attention Maps (CLS Token â†’ Patches)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.colorbar(im, ax=axes, fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Attention Map í•´ì„:\")\n",
    "print(\"   - ë°ì€ ì˜ì—­: ì˜ˆì¸¡ì— ì¤‘ìš”í•œ íŒ¨ì¹˜ (ë†’ì€ attention weight)\")\n",
    "print(\"   - ì–´ë‘ìš´ ì˜ì—­: ëœ ì¤‘ìš”í•œ íŒ¨ì¹˜ (ë‚®ì€ attention weight)\")\n",
    "print(\"   - Layerê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ attentionì´ íŠ¹ì • ì˜ì—­ì— ì§‘ì¤‘í•˜ëŠ” ê²½í–¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš–ï¸ Step 7: Multi-scale Inference\n",
    "\n",
    "ë‹¤ì–‘í•œ ì´ë¯¸ì§€ í¬ê¸°(224, 384, 512)ë¡œ ì¶”ë¡  ì†ë„ ë° ì„±ëŠ¥ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ë‹¤ì–‘í•œ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì¶”ë¡ \n",
    "scales = [224, 384, 512]\n",
    "results = []\n",
    "\n",
    "print(\"ğŸ”„ Multi-scale Inference ì‹¤í–‰ ì¤‘...\\n\")\n",
    "\n",
    "for size in scales:\n",
    "    # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ\n",
    "    img_resized = sample_img.resize((size, size))\n",
    "    \n",
    "    # ì „ì²˜ë¦¬\n",
    "    inputs = processor(images=img_resized, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # ì¶”ë¡  ì‹œê°„ ì¸¡ì •\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    inference_time = (time.time() - start_time) * 1000  # ms\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    top_prob, top_idx = torch.max(probs, dim=-1)\n",
    "    \n",
    "    # íŒ¨ì¹˜ ìˆ˜ ê³„ì‚°\n",
    "    num_patches_scale = (size // config.patch_size) ** 2\n",
    "    \n",
    "    results.append({\n",
    "        'size': size,\n",
    "        'patches': num_patches_scale,\n",
    "        'class': top_idx.item(),\n",
    "        'confidence': top_prob.item(),\n",
    "        'time_ms': inference_time\n",
    "    })\n",
    "    \n",
    "    print(f\"ğŸ“ í¬ê¸° {size}Ã—{size}:\")\n",
    "    print(f\"   - Patch ìˆ˜: {num_patches_scale}ê°œ\")\n",
    "    print(f\"   - ì˜ˆì¸¡ í´ë˜ìŠ¤: {top_idx.item()}\")\n",
    "    print(f\"   - ì‹ ë¢°ë„: {top_prob.item():.2%}\")\n",
    "    print(f\"   - ì¶”ë¡  ì‹œê°„: {inference_time:.2f} ms\\n\")\n",
    "\n",
    "# ê²°ê³¼ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ì¶”ë¡  ì‹œê°„\n",
    "axes[0].bar([str(r['size']) for r in results], [r['time_ms'] for r in results], color='skyblue')\n",
    "axes[0].set_title('Inference Time (ms)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Image Size')\n",
    "axes[0].set_ylabel('Time (ms)')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# ì‹ ë¢°ë„\n",
    "axes[1].bar([str(r['size']) for r in results], [r['confidence'] for r in results], color='coral')\n",
    "axes[1].set_title('Prediction Confidence', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Image Size')\n",
    "axes[1].set_ylabel('Confidence')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Multi-scale ë¶„ì„:\")\n",
    "print(\"   - ì´ë¯¸ì§€ í¬ê¸°ê°€ í´ìˆ˜ë¡ íŒ¨ì¹˜ ìˆ˜ ì¦ê°€ â†’ ì¶”ë¡  ì‹œê°„ ì¦ê°€\")\n",
    "print(\"   - ë” ë§ì€ íŒ¨ì¹˜ë¡œ ë” ì„¸ë°€í•œ íŠ¹ì§• ì¶”ì¶œ ê°€ëŠ¥\")\n",
    "print(\"   - ì‹¤ì‹œê°„ ì¶”ë¡ ì´ í•„ìš”í•˜ë©´ 224Ã—224, ì •í™•ë„ê°€ ì¤‘ìš”í•˜ë©´ 384 ì´ìƒ ê¶Œì¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Step 8: ê²°ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "output_dir = Path('../outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Multi-scale ê²°ê³¼ ì €ì¥\n",
    "results_df = pd.DataFrame(results)\n",
    "results_file = output_dir / '01_vit_multiscale_results.csv'\n",
    "results_df.to_csv(results_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"âœ… Multi-scale ê²°ê³¼ ì €ì¥: {results_file}\")\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì¡° ì •ë³´ ì €ì¥\n",
    "model_info = pd.DataFrame({\n",
    "    'parameter': ['total_params', 'hidden_size', 'attention_heads', 'layers', 'patch_size', 'image_size', 'num_patches'],\n",
    "    'value': [\n",
    "        model.num_parameters(),\n",
    "        config.hidden_size,\n",
    "        config.num_attention_heads,\n",
    "        config.num_hidden_layers,\n",
    "        config.patch_size,\n",
    "        config.image_size,\n",
    "        num_patches\n",
    "    ]\n",
    "})\n",
    "model_info_file = output_dir / '01_vit_model_info.csv'\n",
    "model_info.to_csv(model_info_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"âœ… ëª¨ë¸ ì •ë³´ ì €ì¥: {model_info_file}\")\n",
    "\n",
    "# Attention weights ì €ì¥ (ì²« ë²ˆì§¸ layer)\n",
    "attention_weights = attentions[0][0, 0].detach().cpu().numpy()  # (seq, seq)\n",
    "attention_file = output_dir / '01_vit_attention_layer1.npy'\n",
    "np.save(attention_file, attention_weights)\n",
    "print(f\"âœ… Attention weights ì €ì¥: {attention_file}\")\n",
    "\n",
    "print(\"\\nğŸ‰ ViT ì†Œê°œ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ì •ë¦¬\n",
    "\n",
    "### âœ… ì™„ë£Œí•œ ë‚´ìš©\n",
    "1. ViT ì•„í‚¤í…ì²˜ ì´í•´ ë° ëª¨ë¸ ë¡œë“œ (86M parameters)\n",
    "2. Patch Embedding ê³¼ì • ì‹œê°í™” (16Ã—16 patches)\n",
    "3. Pre-trained ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ ì¶”ë¡ \n",
    "4. 12-layer Attention Map ì‹œê°í™”\n",
    "5. Multi-scale Inference (224, 384, 512) ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "### ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n",
    "\n",
    "- **ViTì˜ í•µì‹¬ ì•„ì´ë””ì–´**:\n",
    "  - ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë¶„í• í•˜ì—¬ ì‹œí€€ìŠ¤ë¡œ ì²˜ë¦¬ (NLP Transformer í™œìš©)\n",
    "  - CNN ëŒ€ì‹  Self-Attentionìœ¼ë¡œ ì „ì—­ì  ê´€ê³„ í•™ìŠµ\n",
    "  - Position Encodingìœ¼ë¡œ íŒ¨ì¹˜ì˜ ê³µê°„ì  ìœ„ì¹˜ ì •ë³´ ë³´ì¡´\n",
    "\n",
    "- **ViT vs CNN**:\n",
    "  - CNN: Inductive bias (locality, translation invariance) ë‚´ì¥\n",
    "  - ViT: ë” ë§ì€ ë°ì´í„° í•„ìš”, í•˜ì§€ë§Œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ë” ë†’ì€ ì„±ëŠ¥\n",
    "  - ViTëŠ” Long-range dependency í•™ìŠµì— ìœ ë¦¬\n",
    "\n",
    "- **Attention ë¶„ì„**:\n",
    "  - ì´ˆê¸° ë ˆì´ì–´: ì „ì—­ì ìœ¼ë¡œ ê³ ë¥´ê²Œ ë¶„ì‚°\n",
    "  - ê¹Šì€ ë ˆì´ì–´: ê°ì²´ ì˜ì—­ì— ì§‘ì¤‘\n",
    "  - CLS í† í°ì´ ëª¨ë“  íŒ¨ì¹˜ ì •ë³´ë¥¼ ì§‘ì•½í•˜ì—¬ ìµœì¢… ë¶„ë¥˜\n",
    "\n",
    "- **ì‹¤ë¬´ í™œìš©**:\n",
    "  - Pre-trained ViTë¡œ Transfer Learning (ë‹¤ìŒ ì‹¤ìŠµì—ì„œ)\n",
    "  - ì œì¡° ë¶ˆëŸ‰ ê²€ì‚¬, ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„ ë“±\n",
    "  - Attention mapìœ¼ë¡œ ëª¨ë¸ í•´ì„ ê°€ëŠ¥ (XAI)\n",
    "\n",
    "### ğŸ“š ë‹¤ìŒ ë‹¨ê³„\n",
    "- **02_vit_transfer_learning.ipynb**: ViT Fine-tuningìœ¼ë¡œ ì œì¡° ë¶ˆëŸ‰ ë¶„ë¥˜\n",
    "- **03_yolov8_finetuning.ipynb**: YOLOv8 Object Detection\n",
    "\n",
    "### ğŸ”— ì°¸ê³  ìë£Œ\n",
    "- [ViT ë…¼ë¬¸](https://arxiv.org/abs/2010.11929) - An Image is Worth 16x16 Words\n",
    "- [Hugging Face ViT](https://huggingface.co/docs/transformers/model_doc/vit)\n",
    "- [Transformers ê³µì‹ ë¬¸ì„œ](https://huggingface.co/docs/transformers/)\n",
    "\n",
    "---\n",
    "\n",
    "*ì œì¡°AI êµìœ¡ v12 Enhanced | Part 2-2 | 2025.02*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}