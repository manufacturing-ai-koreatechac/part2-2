{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ğŸ“ ViT ì „ì´í•™ìŠµ (Transfer Learning)\n",
    "\n",
    "> **í•™ìŠµ ëª©í‘œ**: ì‚¬ì „í•™ìŠµëœ ViT ëª¨ë¸ì„ ì œì¡° ë¶ˆëŸ‰ ë°ì´í„°ë¡œ Fine-tuningí•˜ì—¬ ê³ ì„±ëŠ¥ ë¶„ë¥˜ê¸°ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ í•™ìŠµ ë‚´ìš©\n",
    "\n",
    "1. âœ… ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ (ViT-Base-Patch16-224)\n",
    "2. âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ (Train 70%, Val 15%, Test 15%)\n",
    "3. âœ… ë¶„ë¥˜ í—¤ë“œ êµì²´ (4-class ì œì¡° ë¶ˆëŸ‰)\n",
    "4. âœ… Fine-tuning ì „ëµ (Learning Rate Scheduling)\n",
    "5. âœ… í•™ìŠµ ëª¨ë‹ˆí„°ë§ (Loss, Accuracy Curves)\n",
    "6. âœ… ì„±ëŠ¥ í‰ê°€ (Confusion Matrix, Per-class Metrics)\n",
    "7. âœ… Feature ì‹œê°í™” (t-SNE Embedding)\n",
    "\n",
    "**ì†Œìš” ì‹œê°„**: ì•½ 50ë¶„  \n",
    "**ë‚œì´ë„**: â­â­â­â­ (ê³ ê¸‰)  \n",
    "**ì‚¬ì „ ì§€ì‹**: ë”¥ëŸ¬ë‹ ê¸°ì´ˆ, ì „ì´í•™ìŠµ ê°œë…, PyTorch ì…ë¬¸\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Hugging Face Transformers\n",
    "try:\n",
    "    import transformers\n",
    "    from transformers import (\n",
    "        ViTForImageClassification,\n",
    "        ViTImageProcessor,\n",
    "        Trainer,\n",
    "        TrainingArguments\n",
    "    )\n",
    "    print(f\"âœ… Transformers ë²„ì „: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Transformers ì„¤ì¹˜ í•„ìš”: pip install transformers\")\n",
    "\n",
    "# ì´ë¯¸ì§€ ì²˜ë¦¬\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ë°ì´í„° ë¶„ì„\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# í‰ê°€ ë©”íŠ¸ë¦­\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    accuracy_score, precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ìœ í‹¸ë¦¬í‹°\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# ì‹œë“œ ê³ ì • (ì¬í˜„ì„±)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\nâœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## ğŸ“‚ Step 2: ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "\n",
    "**ë°ì´í„° êµ¬ì¡°**: `defect_images/` ë””ë ‰í† ë¦¬ì— í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ì €ì¥  \n",
    "**ë¶„í• **: Train 70%, Validation 15%, Test 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •\n# 1ìˆœìœ„: KAMP ì‹¤ì œ ë°ì´í„° (ì‹œì§€íŠ¸ë¡œë‹‰ìŠ¤ ì´ë¯¸ì§€ - 6ê°œ ë¶ˆëŸ‰ ìœ í˜•)\n# 2ìˆœìœ„: ë¡œì»¬ data/defect_images í´ë”\n# 3ìˆœìœ„: í•©ì„± ë°ì´í„° ìë™ ìƒì„± (FileNotFoundError ë°©ì§€)\n\nimport tempfile\n\nkamp_dir = Path('../../dataset/part2-2/ì‹œì§€íŠ¸ë¡œë‹‰ìŠ¤_ì´ë¯¸ì§€')\nlocal_dir = Path('../data/defect_images')\n\nif kamp_dir.exists() and any(kamp_dir.iterdir()):\n    data_dir = kamp_dir\n    # KAMP ì‹¤ì œ í´ë˜ìŠ¤: AREA, DISTRIBUTION, FAIL, NEEDLE, PASS, SCATCH\n    classes = sorted([d.name for d in kamp_dir.iterdir() if d.is_dir()])\n    data_source = \"KAMP ì‹¤ì œ ë°ì´í„° (ì‹œì§€íŠ¸ë¡œë‹‰ìŠ¤)\"\nelif local_dir.exists() and any(local_dir.rglob('*.png')) or local_dir.exists() and any(local_dir.rglob('*.jpg')):\n    data_dir = local_dir\n    classes = sorted([d.name for d in local_dir.iterdir() if d.is_dir()])\n    if not classes:\n        classes = ['normal', 'scratch', 'contamination', 'damage']\n    data_source = \"ë¡œì»¬ ë°ì´í„°\"\nelse:\n    # âš ï¸ CRITICAL: í•©ì„± ë°ì´í„° ìƒì„± (ì›ë³¸ ì½”ë“œëŠ” ì—¬ê¸°ì„œ FileNotFoundError ë°œìƒ)\n    print(\"âš ï¸ ì‹¤ì œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•©ì„± í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\")\n    data_dir = Path(tempfile.mkdtemp(prefix='vit_defect_'))\n    classes = ['normal', 'scratch', 'contamination', 'damage']\n    data_source = f\"í•©ì„± ë°ì´í„° (ìë™ ìƒì„±: {data_dir})\"\n    \n    for cls in classes:\n        cls_dir = data_dir / cls\n        cls_dir.mkdir(parents=True, exist_ok=True)\n        # í´ë˜ìŠ¤ë‹¹ 30ì¥ì˜ í•©ì„± ì´ë¯¸ì§€ ìƒì„± (ì´ 120ì¥)\n        for i in range(30):\n            img_array = np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8)\n            # í´ë˜ìŠ¤ë³„ ì‹œê°ì  ì°¨ì´ ë¶€ì—¬\n            if cls == 'scratch':\n                # ëŒ€ê°ì„  ìŠ¤í¬ë˜ì¹˜ íŒ¨í„´\n                for offset in range(-2, 3):\n                    coords = np.arange(224)\n                    img_array[coords, np.clip(coords + offset, 0, 223)] = [255, 255, 255]\n            elif cls == 'contamination':\n                # ì›í˜• ì–¼ë£© íŒ¨í„´\n                y, x = np.ogrid[-112:112, -112:112]\n                mask = x**2 + y**2 <= np.random.randint(20, 50)**2\n                img_array[mask] = [80, 60, 40]\n            elif cls == 'damage':\n                # ë¶ˆê·œì¹™ ë…¸ì´ì¦ˆ íŒ¨í„´\n                noise = np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8)\n                region = slice(50, 174), slice(50, 174)\n                img_array[region] = noise[region]\n            \n            img = Image.fromarray(img_array)\n            img.save(cls_dir / f'{cls}_{i:03d}.png')\n    \n    print(f\"   âœ… í•©ì„± ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(classes)}ê°œ í´ë˜ìŠ¤ Ã— 30ì¥ = 120ì¥\")\n\nnum_classes = len(classes)\n\nprint(f\"ğŸ“‚ ë°ì´í„° ê²½ë¡œ: {data_dir}\")\nprint(f\"ğŸ“¦ ë°ì´í„° ì†ŒìŠ¤: {data_source}\")\nprint(f\"\\nğŸ“Š í´ë˜ìŠ¤ ì •ë³´ ({num_classes}ê°œ):\")\nfor idx, cls in enumerate(classes):\n    cls_dir = data_dir / cls\n    if cls_dir.exists():\n        image_count = len(list(cls_dir.glob('*.png'))) + len(list(cls_dir.glob('*.jpg')))\n        print(f\"   [{idx}] {cls}: {image_count}ê°œ ì´ë¯¸ì§€\")\n    else:\n        print(f\"   [{idx}] {cls}: ë””ë ‰í† ë¦¬ ì—†ìŒ\")\n\nprint(f\"\\nâœ… ë°ì´í„° ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì™„ë£Œ!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset í´ë˜ìŠ¤\n",
    "class DefectDataset(Dataset):\n",
    "    \"\"\"ì œì¡° ë¶ˆëŸ‰ ì´ë¯¸ì§€ ë°ì´í„°ì…‹\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, classes, processor, augment=False):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.classes = classes\n",
    "        self.processor = processor\n",
    "        self.augment = augment\n",
    "        \n",
    "        # ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for idx, cls in enumerate(classes):\n",
    "            cls_dir = self.data_dir / cls\n",
    "            image_files = list(cls_dir.glob('*.jpg')) + list(cls_dir.glob('*.png'))\n",
    "            \n",
    "            for img_path in image_files:\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(idx)\n",
    "        \n",
    "        print(f\"ğŸ“¦ ë°ì´í„°ì…‹ ìƒì„±: {len(self)}ê°œ ìƒ˜í”Œ\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # ì´ë¯¸ì§€ ë¡œë“œ\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Data Augmentation (Training only)\n",
    "        if self.augment:\n",
    "            transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            ])\n",
    "            image = transform(image)\n",
    "        \n",
    "        # ViT Processor ì ìš© (Resize + Normalize)\n",
    "        inputs = self.processor(images=image, return_tensors='pt')\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ViT Image Processor ë¡œë“œ\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "print(f\"âœ… ViT Processor ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"   ì…ë ¥ í¬ê¸°: {processor.size}\")\n",
    "print(f\"   ì •ê·œí™”: mean={processor.image_mean}, std={processor.image_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±\n",
    "full_dataset = DefectDataset(\n",
    "    data_dir=data_dir,\n",
    "    classes=classes,\n",
    "    processor=processor,\n",
    "    augment=False  # ì „ì²´ ë°ì´í„°ì—ëŠ” augmentation ì ìš© ì•ˆí•¨\n",
    ")\n",
    "\n",
    "# Train / Val / Test ë¶„í•  (70% / 15% / 15%)\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š ë°ì´í„° ë¶„í• :\")\n",
    "print(f\"   Train: {len(train_dataset):,}ê°œ ({len(train_dataset)/total_size*100:.1f}%)\")\n",
    "print(f\"   Validation: {len(val_dataset):,}ê°œ ({len(val_dataset)/total_size*100:.1f}%)\")\n",
    "print(f\"   Test: {len(test_dataset):,}ê°œ ({len(test_dataset)/total_size*100:.1f}%)\")\n",
    "\n",
    "# DataLoader ìƒì„±\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nâœ… DataLoader ìƒì„± ì™„ë£Œ! (ë°°ì¹˜ í¬ê¸°: {batch_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 3: ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ ë° ë¶„ë¥˜ í—¤ë“œ êµì²´\n",
    "\n",
    "**ëª¨ë¸**: ViT-Base-Patch16-224 (ImageNet ì‚¬ì „í•™ìŠµ)  \n",
    "**ì „ëµ**: ë¶„ë¥˜ í—¤ë“œë§Œ êµì²´í•˜ì—¬ 4-class ì œì¡° ë¶ˆëŸ‰ ë¶„ë¥˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ ì‚¬ì „í•™ìŠµ ViT ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "# ViT ëª¨ë¸ ë¡œë“œ (ImageNet ì‚¬ì „í•™ìŠµ)\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=num_classes,  # ë¶„ë¥˜ í—¤ë“œ êµì²´ (1000 â†’ 4)\n",
    "    ignore_mismatched_sizes=True  # í¬ê¸° ë¶ˆì¼ì¹˜ ë¬´ì‹œ\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ì„ GPUë¡œ ì´ë™\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\nâœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"   ëª¨ë¸ëª…: ViT-Base-Patch16-224\")\n",
    "print(f\"   íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì¡° í™•ì¸\n",
    "print(f\"\\nğŸ“Š ëª¨ë¸ êµ¬ì¡°:\")\n",
    "print(f\"   ViT Encoder: 12 layers\")\n",
    "print(f\"   Hidden size: 768\")\n",
    "print(f\"   Attention heads: 12\")\n",
    "print(f\"   Patch size: 16Ã—16\")\n",
    "print(f\"   Image size: 224Ã—224\")\n",
    "print(f\"   ë¶„ë¥˜ í—¤ë“œ: {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 4: Fine-tuning ì„¤ì •\n",
    "\n",
    "**í•™ìŠµë¥  ì „ëµ**: Warmup + Cosine Decay  \n",
    "**ìµœì í™”**: AdamW with weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Training Arguments\ntraining_args = TrainingArguments(\n    output_dir='../outputs/vit_finetuned',\n    \n    # í•™ìŠµ ì„¤ì •\n    num_train_epochs=10,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    \n    # ìµœì í™”\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    warmup_ratio=0.1,  # 10% warmup\n    lr_scheduler_type='cosine',  # Cosine decay\n    \n    # í‰ê°€\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    metric_for_best_model='accuracy',\n    \n    # ë¡œê¹…\n    logging_dir='../outputs/logs',\n    logging_steps=10,\n    \n    # ê¸°íƒ€\n    seed=42,\n    fp16=torch.cuda.is_available(),  # Mixed precision (GPU only)\n    dataloader_num_workers=0,  # macOS + Jupyter í™˜ê²½ í˜¸í™˜ (fork ì¶©ëŒ ë°©ì§€)\n)\n\nprint(\"ğŸ“‹ Training Arguments:\")\nprint(f\"   Epochs: {training_args.num_train_epochs}\")\nprint(f\"   Learning Rate: {training_args.learning_rate}\")\nprint(f\"   Batch Size: {training_args.per_device_train_batch_size}\")\nprint(f\"   Weight Decay: {training_args.weight_decay}\")\nprint(f\"   Warmup Ratio: {training_args.warmup_ratio}\")\nprint(f\"   LR Scheduler: {training_args.lr_scheduler_type}\")\nprint(f\"   FP16: {training_args.fp16}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê°€ ë©”íŠ¸ë¦­ í•¨ìˆ˜\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"ì •í™•ë„ ë° F1 ê³„ì‚°\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # ì •í™•ë„\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Precision, Recall, F1 (macro average)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Trainer ìƒì„±\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Trainer ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"   ì´ í•™ìŠµ ìŠ¤í…: {len(train_loader) * training_args.num_train_epochs:,}\")\n",
    "print(f\"   Warmup ìŠ¤í…: {int(len(train_loader) * training_args.num_train_epochs * training_args.warmup_ratio):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## ğŸš€ Step 5: ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "**ì£¼ì˜**: ì‹¤ì œ í•™ìŠµì€ GPU í™˜ê²½ì—ì„œ ìˆ˜í–‰ ê¶Œì¥ (ì•½ 10-20ë¶„ ì†Œìš”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ Fine-tuning ì‹œì‘...\\n\")\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "train_result = trainer.train()\n",
    "\n",
    "# í•™ìŠµ ê²°ê³¼\n",
    "print(\"\\nâœ… Fine-tuning ì™„ë£Œ!\")\n",
    "print(f\"\\nğŸ“Š í•™ìŠµ ê²°ê³¼:\")\n",
    "print(f\"   ìµœì¢… Loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   í•™ìŠµ ì‹œê°„: {train_result.metrics['train_runtime']:.1f}ì´ˆ\")\n",
    "print(f\"   í‰ê·  ì†ë„: {train_result.metrics['train_samples_per_second']:.1f} samples/sec\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "trainer.save_model('../outputs/vit_finetuned_best')\n",
    "print(f\"\\nğŸ’¾ ëª¨ë¸ ì €ì¥: ../outputs/vit_finetuned_best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 6: í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "\n",
    "**Loss & Accuracy** ì¶”ì´ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ë¡œê·¸ ì¶”ì¶œ\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Train/Val Loss ì¶”ì¶œ\n",
    "train_loss = [log['loss'] for log in log_history if 'loss' in log]\n",
    "eval_loss = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
    "eval_accuracy = [log['eval_accuracy'] for log in log_history if 'eval_accuracy' in log]\n",
    "\n",
    "epochs = range(1, len(eval_loss) + 1)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Loss Curve\n",
    "axes[0].plot(train_loss, label='Train Loss', linewidth=2, color='skyblue')\n",
    "axes[0].plot(\n",
    "    np.linspace(0, len(train_loss), len(eval_loss)),\n",
    "    eval_loss,\n",
    "    label='Val Loss',\n",
    "    linewidth=2,\n",
    "    color='coral',\n",
    "    marker='o'\n",
    ")\n",
    "axes[0].set_xlabel('Training Steps', fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontweight='bold')\n",
    "axes[0].set_title('Loss Curve', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy Curve\n",
    "axes[1].plot(epochs, eval_accuracy, linewidth=2, color='green', marker='o')\n",
    "axes[1].set_xlabel('Epoch', fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy', fontweight='bold')\n",
    "axes[1].set_title('Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/02_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ í•™ìŠµ ê³¡ì„ :\")\n",
    "print(f\"   ìµœì¢… Train Loss: {train_loss[-1]:.4f}\")\n",
    "print(f\"   ìµœì¢… Val Loss: {eval_loss[-1]:.4f}\")\n",
    "print(f\"   ìµœê³  Val Accuracy: {max(eval_accuracy):.4f} (Epoch {np.argmax(eval_accuracy)+1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 7: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€\n",
    "\n",
    "**Confusion Matrix & í´ë˜ìŠ¤ë³„ ì„±ëŠ¥**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ ì¤‘...\\n\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# ì „ì²´ ì •í™•ë„\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"ğŸ“Š í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ ì„±ëŠ¥\n",
    "print(f\"\\nğŸ“‹ Classification Report:\")\n",
    "report = classification_report(y_true, y_pred, target_names=classes, digits=4)\n",
    "print(report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw Counts\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=classes,\n",
    "    yticklabels=classes,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_xlabel('Predicted', fontweight='bold')\n",
    "axes[0].set_ylabel('Actual', fontweight='bold')\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Normalized\n",
    "sns.heatmap(\n",
    "    cm_normalized,\n",
    "    annot=True,\n",
    "    fmt='.2%',\n",
    "    cmap='Greens',\n",
    "    xticklabels=classes,\n",
    "    yticklabels=classes,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_xlabel('Predicted', fontweight='bold')\n",
    "axes[1].set_ylabel('Actual', fontweight='bold')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/02_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## ğŸ¨ Step 8: Feature ì‹œê°í™” (t-SNE)\n",
    "\n",
    "**ViTì˜ ë§ˆì§€ë§‰ Hidden State**ë¥¼ t-SNEë¡œ 2D ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ Feature Extraction ì¤‘...\\n\")\n",
    "\n",
    "# Feature ì¶”ì¶œ í•¨ìˆ˜\n",
    "def extract_features(model, dataloader, device):\n",
    "    \"\"\"ViTì˜ ë§ˆì§€ë§‰ hidden state ì¶”ì¶œ\"\"\"\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            batch_labels = batch['labels'].cpu().numpy()\n",
    "            \n",
    "            # Forward pass (ë§ˆì§€ë§‰ hidden state ì¶”ì¶œ)\n",
    "            outputs = model.vit(\n",
    "                pixel_values=pixel_values,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            \n",
    "            # [CLS] tokenì˜ hidden state ì‚¬ìš©\n",
    "            cls_hidden_state = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            \n",
    "            features.append(cls_hidden_state)\n",
    "            labels.append(batch_labels)\n",
    "    \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ Feature ì¶”ì¶œ\n",
    "test_features, test_labels = extract_features(model, test_loader, device)\n",
    "\n",
    "print(f\"âœ… Feature Extraction ì™„ë£Œ!\")\n",
    "print(f\"   Feature Shape: {test_features.shape}\")\n",
    "print(f\"   Feature Dimension: {test_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ t-SNE ì°¨ì› ì¶•ì†Œ ì¤‘... (ì•½ 1-2ë¶„ ì†Œìš”)\\n\")\n",
    "\n",
    "# t-SNE ì ìš©\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    n_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "features_2d = tsne.fit_transform(test_features)\n",
    "\n",
    "print(f\"âœ… t-SNE ì™„ë£Œ!\")\n",
    "print(f\"   2D Embedding Shape: {features_2d.shape}\")\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "for idx, cls in enumerate(classes):\n",
    "    mask = test_labels == idx\n",
    "    plt.scatter(\n",
    "        features_2d[mask, 0],\n",
    "        features_2d[mask, 1],\n",
    "        c=colors[idx],\n",
    "        label=cls,\n",
    "        alpha=0.6,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12, fontweight='bold')\n",
    "plt.title('ViT Feature Space (t-SNE)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/02_tsne_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ í•´ì„:\")\n",
    "print(f\"   - ê°™ì€ í´ë˜ìŠ¤ê°€ ê°€ê¹Œì´ ëª¨ì—¬ ìˆìœ¼ë©´ íŠ¹ì§• í•™ìŠµì´ ì˜ ëœ ê²ƒ\")\n",
    "print(f\"   - ë‹¤ë¥¸ í´ë˜ìŠ¤ê°€ ëª…í™•íˆ ë¶„ë¦¬ë˜ë©´ ë¶„ë¥˜ ì„±ëŠ¥ì´ ì¢‹ìŒ\")\n",
    "print(f\"   - í´ëŸ¬ìŠ¤í„° ê²¹ì¹¨ì´ ìˆìœ¼ë©´ í˜¼ë™ ê°€ëŠ¥ì„± ì¡´ì¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Step 9: ê²°ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "output_dir = Path('../outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. í•™ìŠµ íˆìŠ¤í† ë¦¬\n",
    "history_df = pd.DataFrame({\n",
    "    'epoch': epochs,\n",
    "    'val_loss': eval_loss,\n",
    "    'val_accuracy': eval_accuracy\n",
    "})\n",
    "history_file = output_dir / '02_training_history.csv'\n",
    "history_df.to_csv(history_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"âœ… í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥: {history_file}\")\n",
    "\n",
    "# 2. í…ŒìŠ¤íŠ¸ ê²°ê³¼\n",
    "test_results = pd.DataFrame({\n",
    "    'class': classes,\n",
    "    'precision': precision_recall_fscore_support(y_true, y_pred, average=None)[0],\n",
    "    'recall': precision_recall_fscore_support(y_true, y_pred, average=None)[1],\n",
    "    'f1_score': precision_recall_fscore_support(y_true, y_pred, average=None)[2],\n",
    "    'support': precision_recall_fscore_support(y_true, y_pred, average=None)[3]\n",
    "})\n",
    "test_results_file = output_dir / '02_test_results.csv'\n",
    "test_results.to_csv(test_results_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"âœ… í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {test_results_file}\")\n",
    "\n",
    "# 3. Confusion Matrix\n",
    "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "cm_file = output_dir / '02_confusion_matrix.csv'\n",
    "cm_df.to_csv(cm_file, encoding='utf-8-sig')\n",
    "print(f\"âœ… Confusion Matrix ì €ì¥: {cm_file}\")\n",
    "\n",
    "# 4. t-SNE Features\n",
    "tsne_df = pd.DataFrame({\n",
    "    'tsne_dim1': features_2d[:, 0],\n",
    "    'tsne_dim2': features_2d[:, 1],\n",
    "    'label': test_labels,\n",
    "    'class_name': [classes[label] for label in test_labels]\n",
    "})\n",
    "tsne_file = output_dir / '02_tsne_features.csv'\n",
    "tsne_df.to_csv(tsne_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"âœ… t-SNE Features ì €ì¥: {tsne_file}\")\n",
    "\n",
    "print(\"\\nğŸ‰ ëª¨ë“  ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ì •ë¦¬\n",
    "\n",
    "### âœ… ì™„ë£Œí•œ ë‚´ìš©\n",
    "\n",
    "1. **ì‚¬ì „í•™ìŠµ ëª¨ë¸ í™œìš©** - ImageNetìœ¼ë¡œ ì‚¬ì „í•™ìŠµëœ ViT-Base ë¡œë“œ\n",
    "2. **ë¶„ë¥˜ í—¤ë“œ êµì²´** - 1000-class â†’ 4-class ì œì¡° ë¶ˆëŸ‰ ë¶„ë¥˜\n",
    "3. **ë°ì´í„° ë¶„í• ** - Train 70%, Validation 15%, Test 15%\n",
    "4. **Fine-tuning ì „ëµ** - Learning Rate Scheduling (Warmup + Cosine Decay)\n",
    "5. **í•™ìŠµ ëª¨ë‹ˆí„°ë§** - Loss & Accuracy Curves\n",
    "6. **ì„±ëŠ¥ í‰ê°€** - Confusion Matrix, Per-class Metrics\n",
    "7. **Feature ì‹œê°í™”** - t-SNEë¡œ 2D Embedding\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n",
    "\n",
    "#### ì „ì´í•™ìŠµì˜ ì¥ì \n",
    "\n",
    "**ì‚¬ì „í•™ìŠµ ëª¨ë¸ í™œìš©**:\n",
    "- âœ… ì ì€ ë°ì´í„°ë¡œ ê³ ì„±ëŠ¥ ë‹¬ì„± (ìˆ˜ë°±~ìˆ˜ì²œ ì¥)\n",
    "- âœ… í•™ìŠµ ì‹œê°„ ë‹¨ì¶• (ì²˜ìŒë¶€í„° í•™ìŠµ ëŒ€ë¹„ 10ë°° ì´ìƒ)\n",
    "- âœ… ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ (ImageNet ì§€ì‹ ì „ì´)\n",
    "\n",
    "**Fine-tuning ì „ëµ**:\n",
    "- **Learning Rate**: ì‚¬ì „í•™ìŠµë³´ë‹¤ ë‚®ê²Œ (2e-4 ~ 5e-5)\n",
    "- **Warmup**: ì´ˆê¸° ë¶ˆì•ˆì •ì„± ë°©ì§€ (10%)\n",
    "- **Cosine Decay**: í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ\n",
    "- **Weight Decay**: ê³¼ì í•© ë°©ì§€ (0.01)\n",
    "\n",
    "---\n",
    "\n",
    "#### ViT vs CNN ë¹„êµ\n",
    "\n",
    "**ViTì˜ ì¥ì **:\n",
    "- âœ… ì¥ê±°ë¦¬ ì˜ì¡´ì„± í•™ìŠµ (Global attention)\n",
    "- âœ… í™•ì¥ì„± (ë” í° ëª¨ë¸ë¡œ ì„±ëŠ¥ í–¥ìƒ)\n",
    "- âœ… í•´ì„ ê°€ëŠ¥ì„± (Attention map ë¶„ì„)\n",
    "\n",
    "**ViTì˜ ë‹¨ì **:\n",
    "- âŒ ë§ì€ ë°ì´í„° í•„ìš” (ì „ì´í•™ìŠµìœ¼ë¡œ í•´ê²°)\n",
    "- âŒ ê³„ì‚° ë¹„ìš© ë†’ìŒ (GPU í•„ìˆ˜)\n",
    "- âŒ Inductive bias ë¶€ì¡± (CNNì€ locality ë‚´ì¥)\n",
    "\n",
    "---\n",
    "\n",
    "#### ì‹¤ë¬´ ì ìš© ê°€ì´ë“œ\n",
    "\n",
    "**ë°ì´í„° í¬ê¸°ë³„ ì „ëµ**:\n",
    "- **ì†Œê·œëª¨ (<1K)**: ê°•í•œ Data Augmentation + Freeze ì¼ë¶€ ë ˆì´ì–´\n",
    "- **ì¤‘ê·œëª¨ (1K-10K)**: ì „ì²´ Fine-tuning (ì´ ì‹¤ìŠµ)\n",
    "- **ëŒ€ê·œëª¨ (>10K)**: Scratch í•™ìŠµ ê³ ë ¤\n",
    "\n",
    "**ì„±ëŠ¥ ê°œì„  íŒ**:\n",
    "1. **Data Augmentation**: ì œì¡° ë„ë©”ì¸ íŠ¹í™” (íšŒì „, ì¡°ëª… ë³€í™”)\n",
    "2. **Class Imbalance**: ê°€ì¤‘ì¹˜ ì¡°ì • ë˜ëŠ” Oversampling\n",
    "3. **Ensemble**: ì—¬ëŸ¬ ëª¨ë¸ ê²°í•©ìœ¼ë¡œ robustness í–¥ìƒ\n",
    "4. **Test-Time Augmentation**: ì˜ˆì¸¡ ì‹œ augmentation ì ìš©\n",
    "\n",
    "**ë°°í¬ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸**:\n",
    "- âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„ 90% ì´ìƒ\n",
    "- âœ… í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ê· í˜• (Recall ì°¨ì´ <10%)\n",
    "- âœ… ì¶”ë¡  ì†ë„ ëª©í‘œ ë‹¬ì„± (<100ms/image)\n",
    "- âœ… ëª¨ë¸ ê²½ëŸ‰í™” (Quantization, Pruning)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "- **Part 2-2**: YOLOv8 ê°ì²´ íƒì§€ Fine-tuning\n",
    "- **Part 3-1**: í•œêµ­ì–´ ì„ë² ë”© ë° RAG ì‹œìŠ¤í…œ\n",
    "\n",
    "### ğŸ”— ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [ViT ë…¼ë¬¸](https://arxiv.org/abs/2010.11929) - An Image is Worth 16x16 Words\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)\n",
    "- [Transfer Learning Guide](https://ruder.io/transfer-learning/)\n",
    "\n",
    "---\n",
    "\n",
    "*ì œì¡°AI êµìœ¡ v12 Enhanced | Part 2-2 | 2025.02*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}